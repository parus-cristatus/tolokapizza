{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parus-cristatus/tolokapizza/blob/main/tolokaapi/project_transfer_between_accounts_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Refactored script to transfer indicated project with regular pools and trainings between production accounts**\n",
        "\n",
        "1. The script doesn't transfer uploaded tasks or task suites.\n",
        "\n",
        "2. Transfered skills and trainings mappings are saved to .csv files to persist the information and prevent duplicates. Skills are account-wide while trainings are project-wide.\n",
        "\n",
        "3. Global skills are transfered with the same IDs.\n",
        "\n",
        "4. Regular pool types (Exam, Rehab, Other and Training) can't be tansfered via API, thus must be rechecked manually.\n",
        "\n",
        "**Important:**\n",
        "- Trainings must have smart mixing configuration set on them. Otherwise the script will report everything is OK but they'll not be transfered. The situation can happen if you create a training in UI and don't upload any tasks.\n",
        "- It's more reliable to transfer projects one by one, check mapping files and check results in UI.\n",
        "\n"
      ],
      "metadata": {
        "id": "cVADskHE3BB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vym2KEdAmNz_"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "from typing import Generator, Tuple, List, Set, Dict, Callable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROD_ENDPOINT = 'https://toloka.dev/api/v1/'\n",
        "TOKEN_FROM = '12345' # API token to interact with the account you need to transfer a project from\n",
        "TOKEN_TO = '67890' # API token to interact with the account you need to transfer a project to\n",
        "TOKEN_PREFIX = 'ApiKey'"
      ],
      "metadata": {
        "id": "wfa6OVm0mV7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Current pool json body keys that contain skills for recursive walk\n",
        "POOL_FIELDS_WITH_SKILLS = ['key', 'answer_weight_skill_id', 'skill_id']"
      ],
      "metadata": {
        "id": "RpdbbTQlaHsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_call(endpoint: str, call: str, token: str, params: dict=None) -> dict:\n",
        "    \"\"\"Returns json data from get request.\n",
        "\n",
        "    Arguments:\n",
        "        call: parameter\n",
        "        token: toloka token (sandbox or prod)\n",
        "        endpoint: endpoint (sandbox or prod)\n",
        "        params: get call params\n",
        "\n",
        "    Examples:\n",
        "        >>> endpoint_from = 'https://toloka.dev/api/v1/'\n",
        "        >>> token_from = 'KpC_UWW4h17E...'\n",
        "        >>> get_call('projects', token_from, endpoint_from, {'limit': 100})\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": TOKEN_PREFIX + ' ' + token\n",
        "    }\n",
        "    response = requests.get(url=f\"{endpoint}{call}\", headers=headers, params=params)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "C48mJHBAm6R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_call(endpoint: str, call: str, token: str, data: str) -> dict:\n",
        "    \"\"\"Returns json data from post request.\n",
        "\n",
        "    Arguments:\n",
        "        call: parameter\n",
        "        endpoint: endpoint (sandbox or prod)\n",
        "        token: toloka token (sandbox or prod)\n",
        "        data: data to send in the body of the request\n",
        "\n",
        "    Examples:\n",
        "        >>> endpoint_to = 'https://sandbox.toloka.dev/api/v1/'\n",
        "        >>> token_to = 'HQGC_xPF6W...'\n",
        "        >>> post_call('projects', endpoint_to, token_to, json.dumps(project_from))\n",
        "    \"\"\"\n",
        "    headers={\n",
        "        \"Authorization\": TOKEN_PREFIX + ' ' + token,\n",
        "        \"Content-Type\": \"application/JSON\"\n",
        "    }\n",
        "    response = requests.post(f\"{endpoint}{call}\", data=data, headers=headers)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "A9HUbw7IpUYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_values_from_dict(keys: list, data: dict) -> Generator[List[str], None, None]:\n",
        "    \"\"\"Returns generator\n",
        "\n",
        "    Arguments:\n",
        "        keys: list keys by which looking for values\n",
        "        data: dict from which get the values\n",
        "\n",
        "    Examples:\n",
        "        >>> get_values_from_dict(['key_id','skill_id'], data)\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        for k, v in data.items():\n",
        "            if k in keys and v.isdigit():\n",
        "                yield v\n",
        "            if isinstance(v, (dict, list)):\n",
        "                yield from get_values_from_dict(keys, v)\n",
        "    elif isinstance(data, list):\n",
        "        for i in data:\n",
        "            if i.get('key') in keys:\n",
        "                yield i.get('value')\n",
        "            yield from get_values_from_dict(keys, i)"
      ],
      "metadata": {
        "id": "9sLeoMcoxExf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_upd_dict(data: dict, values: dict) -> dict:\n",
        "    \"\"\"Returns dict with updated values.\n",
        "\n",
        "    Arguments:\n",
        "        data: dict in which to update the values\n",
        "        values: dict with new values\n",
        "\n",
        "    Examples:\n",
        "        >>> data = {\"filter\": {'or': [{'category': 'skill', 'key': '29716', 'operator': 'NE'}]}}\n",
        "        >>> values = {'29716': '11659'}\n",
        "        >>> deep_upd_dict(data, values)\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        return {k: deep_upd_dict(v, values) for k, v in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [deep_upd_dict(x, values) for x in data]\n",
        "    else:\n",
        "        return values.get(data, data)"
      ],
      "metadata": {
        "id": "1hpYRVKnxIVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pools(prj_id: str, status: str, endpoint: str, token: str) -> list:\n",
        "    \"\"\"Returns list of pools\n",
        "\n",
        "    Arguments:\n",
        "        prj_id: project id\n",
        "        status: pool status (OPEN, CLOSED, ARCHIVED)\n",
        "        endpoint: endpoint (sandbox or prod)\n",
        "        token: toloka token (sandbox or prod)\n",
        "    \"\"\"\n",
        "    params = {\n",
        "            'project_id': prj_id,\n",
        "            'status': status.upper()\n",
        "        }\n",
        "    return get_call(endpoint, 'pools', token, params)['items']"
      ],
      "metadata": {
        "id": "so-3LA2A110M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_skill(endpoint: str, skill_id: str, token: str) -> dict:\n",
        "    \"\"\"Returns dict with data the requested skill\n",
        "\n",
        "    Arguments:\n",
        "        endpoint: endpoint (sandbox or prod)\n",
        "        skill_id: skill id\n",
        "        token: toloka token (sandbox or prod)\n",
        "    \"\"\"\n",
        "    return get_call(endpoint, f'skills/{skill_id}', token)"
      ],
      "metadata": {
        "id": "dneej8610AUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_skill(data: str, endpoint: str, token: str) -> dict:\n",
        "    return post_call(endpoint, 'skills', token, data=json.dumps(data))"
      ],
      "metadata": {
        "id": "mRXpgFkN2Lc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_existing_mappings(file_path: str):\n",
        "    existing_mappings = {}\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, 'r', newline='') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                existing_mappings[row['from_id']] = row['to_id']\n",
        "    return existing_mappings"
      ],
      "metadata": {
        "id": "c4n3XhtVTGpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_mappings_to_csv(file_path: str, new_val: dict):\n",
        "    fieldnames = ['from_id', 'to_id']\n",
        "    file_exists = os.path.isfile(file_path)\n",
        "    with open(file_path, 'a', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "        for from_id, to_id in new_val.items():\n",
        "            writer.writerow({'from_id': from_id, 'to_id': to_id})"
      ],
      "metadata": {
        "id": "J-KhQv5FTLp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure training pool has mixer_config parameter set! The script will silently fail to transfer it if there's no such parameter.\n",
        "\n",
        "def handle_trainings(pool_to_transfer: dict):\n",
        "        trainings_file_path = f'trainings_map_project_{PROJECT_ID}.csv'\n",
        "        training_map = {}\n",
        "        if 'quality_control' in pool_to_transfer and pool_to_transfer['quality_control'].get('training_requirement'):\n",
        "            training_from_id = pool_to_transfer['quality_control'].get('training_requirement')['training_pool_id']\n",
        "            existing_training_mappings = read_existing_mappings(trainings_file_path)\n",
        "            if training_from_id in existing_training_mappings:\n",
        "                pool_to_transfer['quality_control']['training_requirement']['training_pool_id'] = existing_training_mappings[training_from_id]\n",
        "            else:\n",
        "                try:\n",
        "                    training_from = get_call(PROD_ENDPOINT, f\"trainings/{training_from_id}\", TOKEN_FROM)\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Failed to get training from {pool_to_transfer['id']} : {e}\")\n",
        "                training_from.pop('id')\n",
        "                training_from['project_id'] = project_to['id']\n",
        "                try:\n",
        "                    training_to = post_call(PROD_ENDPOINT, 'trainings', TOKEN_TO, data=json.dumps(training_from))\n",
        "                    print(f\"Training {training_from_id} was successfully transfered\")\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Failed to create training from {pool_to_transfer['id']} : {e}\")\n",
        "                training_to_id = training_to.get('id')\n",
        "                pool_to_transfer['quality_control']['training_requirement']['training_pool_id'] = training_to_id\n",
        "                training_map[training_from_id] = training_to_id\n",
        "                write_mappings_to_csv(trainings_file_path, training_map)"
      ],
      "metadata": {
        "id": "zvcrjvQ0ovuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_new_skill_mappings(skill_ids: set, existing_skill_mappings: dict, create_skill: callable):\n",
        "        new_val = {}\n",
        "        for i in skill_ids:\n",
        "            if i in existing_skill_mappings:\n",
        "                new_val[i] = existing_skill_mappings[i]\n",
        "            else:\n",
        "                try:\n",
        "                    skill = get_skill(PROD_ENDPOINT, i, TOKEN_FROM)\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Failed to get skill {skill['id']}: {e}\")\n",
        "                if not skill[\"global\"]:\n",
        "                    try:\n",
        "                        new_val[i] = create_skill(skill, PROD_ENDPOINT, TOKEN_TO)['id']\n",
        "                        print(f\"Skill {skill['id']} was successfully transfered\")\n",
        "                    except requests.exceptions.RequestException as e:\n",
        "                        print(f\"Failed to create skill for {skill['id']} : {e}\")\n",
        "        return new_val"
      ],
      "metadata": {
        "id": "YbCcXnw7XgcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_skills(pool_to_transfer: dict):\n",
        "    skill_ids = set(get_values_from_dict(POOL_FIELDS_WITH_SKILLS, pool_to_transfer))\n",
        "    skills_file_path = f'skills_map.csv'\n",
        "    existing_skill_mappings = read_existing_mappings(skills_file_path)\n",
        "    new_val = fetch_new_skill_mappings(skill_ids, existing_skill_mappings, create_skill)\n",
        "    write_mappings_to_csv(skills_file_path, new_val)\n",
        "    return new_val"
      ],
      "metadata": {
        "id": "KQmhqRzZo1Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_pool_for_transfer(pool_id: str):\n",
        "    try:\n",
        "        pool_to_transfer = get_call(PROD_ENDPOINT, f\"pools/{pool_id}\", TOKEN_FROM)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to get pool {pool_id} : {e}\")\n",
        "    pool_to_transfer.pop('id')\n",
        "    pool_to_transfer['project_id'] = project_to['id']\n",
        "\n",
        "    handle_trainings(pool_to_transfer)\n",
        "\n",
        "    upd_pool = deep_upd_dict(pool_to_transfer, handle_skills(pool_to_transfer))\n",
        "\n",
        "    return upd_pool"
      ],
      "metadata": {
        "id": "T4FeSznZT1yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indicate project ID and regular pools IDs to transfer\n",
        "\n",
        "PROJECT_ID = '151522'\n",
        "POOLS = ['42026971', '42026974', '42040746']"
      ],
      "metadata": {
        "id": "efAOxmg4v7oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer project\n",
        "\n",
        "project_from = get_call(PROD_ENDPOINT, f\"projects/{PROJECT_ID}\", TOKEN_FROM)\n",
        "project_to = post_call(PROD_ENDPOINT, 'projects', TOKEN_TO, data=json.dumps(project_from))\n",
        "print(f\"Project {project_from['id']} was successfully transfered\")"
      ],
      "metadata": {
        "id": "5B5rn8UTeHyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer pools\n",
        "\n",
        "for pool in POOLS:\n",
        "    try:\n",
        "        post_call(PROD_ENDPOINT, 'pools', TOKEN_TO, data=json.dumps(prepare_pool_for_transfer(pool)))\n",
        "        print(f\"Pool {pool} was successfully transfered\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to create pool for {pool} : {e}\")"
      ],
      "metadata": {
        "id": "osOOt7W0Uxk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Made with üòÆ‚Äçüí® by Sigma"
      ],
      "metadata": {
        "id": "QNWajoV9p3_z"
      }
    }
  ]
}